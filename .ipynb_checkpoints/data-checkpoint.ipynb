{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import demoji\n",
    "\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('snowball_data')\n",
    "# demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def search(query: str):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\") \n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Navigate to Google\n",
    "    driver.get(\"https://www.google.com\")\n",
    "\n",
    "    # Wait for the search box to be visible\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    search_box = wait.until(EC.visibility_of_element_located((By.NAME, \"q\")))\n",
    "\n",
    "    # Enter the query and submit\n",
    "    search_box.send_keys(f\"{query} review site: youtube.com\")\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # Get the top 5 links\n",
    "    links = driver.find_elements(By.XPATH, \"//div[@class='ct3b9e']//a\")\n",
    "    urls = [link.get_attribute(\"href\") for link in links[:5]]\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"samsung s23 ultra\"\n",
    "urls = search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComments(url):\n",
    "    \n",
    "    # Set up Chrome driver options\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument(\"--headless\") \n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    # Initialize ChromeDriver with an implicit wait of 10 seconds\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.implicitly_wait(20)\n",
    "\n",
    "    # Open the video page\n",
    "    driver.get(url)\n",
    "    sleep(15)\n",
    "\n",
    "    # Wait for the comment section to load\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"continuations\"]')))\n",
    "\n",
    "    # Scroll down to the bottom of the page to load all comments\n",
    "    driver.execute_script('window.scrollTo(1, document.body.scrollHeight);')\n",
    "    sleep(7)\n",
    "    comment_list = []\n",
    "\n",
    "    # Find the comment div element and extract the comments\n",
    "    try:\n",
    "        comment_div = driver.find_element(By.XPATH, '//*[@id=\"contents\"]')\n",
    "        comments = comment_div.find_elements(By.XPATH, '//*[@id=\"content-text\"]')\n",
    "        for comment in comments:\n",
    "            comment_list.append(comment.text.strip())\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find comment section.\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        return comment_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments = []\n",
    "for url in tqdm(urls):\n",
    "    sleep(10)\n",
    "    com = getComments(url)\n",
    "    comments.extend(com)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'comments':comments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('comments.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, device_name = query):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove device name\n",
    "    device_name = device_name.lower()\n",
    "    device_name_terms = [term.strip() for term in device_name.split()]\n",
    "    device_name_regex = '|'.join(device_name_terms)\n",
    "    text = re.sub(device_name_regex, '', text)\n",
    "\n",
    "    # Remove terms related to device\n",
    "    remove_terms = ['phone', 'review', 'display', 'use', 'upgrade', 'screen'] + device_name_terms\n",
    "    remove_terms_regex = '|'.join(remove_terms)\n",
    "    text = re.sub(remove_terms_regex, '', text)\n",
    "\n",
    "    # Remove punctuation marks\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers and special characters\n",
    "    text = re.sub('[^a-zA-Z ]', '', text)\n",
    "\n",
    "    # Remove URLs and mentions\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stem the words\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join the words back into a string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return demoji.replace(text, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_comments'] = df.comments.apply(clean_text)\n",
    "df['cleaned_comments'] = df.cleaned_comments.apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of the sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply the sentiment analyzer to each comment\n",
    "df['sentiment_scores'] = df['cleaned_comments'].apply(lambda x: sid.polarity_scores(x))\n",
    "\n",
    "# Extract the compound score from the sentiment scores\n",
    "df['compound_score'] = df['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Categorize the comments as positive, negative, or neutral based on the compound score\n",
    "df['sentiment_category'] = pd.cut(df['compound_score'], bins=3, labels=['negative', 'neutral', 'positive'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Join the words in the cleaned comments to a single string\n",
    "text = ' '.join(df['cleaned_comments'])\n",
    "\n",
    "# Create a word cloud object and generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='black').generate(text)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
