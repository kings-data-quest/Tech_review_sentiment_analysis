{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import demoji\n",
    "\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_14912\\665898024.py:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "demoji.download_codes()\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " def getComments(url):\n",
    "\n",
    "    # Set up Chrome driver options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    # Initialize ChromeDriver with an implicit wait of 20 seconds\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.implicitly_wait(50)\n",
    "\n",
    "    # Open the video page\n",
    "    driver.get(url)\n",
    "    sleep(7)\n",
    "\n",
    "    # Wait for the comment section to load\n",
    "    wait = WebDriverWait(driver, 40)\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"continuations\"]')))\n",
    "\n",
    "    # Scroll down to the bottom of the page to load all comments\n",
    "    driver.execute_script('window.scrollTo(1, document.body.scrollHeight);')\n",
    "    comment_list = []\n",
    "\n",
    "    # Find the comment div element and extract the comments\n",
    "    try:\n",
    "        comment_div = driver.find_element(By.XPATH, '//*[@id=\"contents\"]')\n",
    "        comments = comment_div.find_elements(By.XPATH, '//*[@id=\"content-text\"]')\n",
    "        for comment in comments:\n",
    "            comment_list.append(comment)\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find comment section.\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        return comment_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.youtube.com/watch?v=zz70o2Ia4X0\", \"https://www.youtube.com/watch?v=XUkV3RsjGrs\",\"https://www.youtube.com/watch?v=UORoRpW8tnA\" ,\n",
    "        \"https://www.youtube.com/watch?v=CM7TbmdOeAs\", \"https://youtu.be/zhoTX0RRXPQ\", \"https://youtu.be/BZPV3Cf3pLo\",\"https://youtu.be/nlowwv-I81M\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments = []\n",
    "# for url in tqdm(urls):\n",
    "#     com = getComments(url)\n",
    "#     comments.extend(com)\n",
    "#     sleep(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'comments':comments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('comments.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation marks\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers and special characters\n",
    "    text = re.sub('[^a-zA-Z ]', '', text)\n",
    "\n",
    "    # Remove URLs and mentions\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stem the words\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join the words back into a string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return demoji.replace(text, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_comments'] = df.comments.apply(clean_text)\n",
    "df['cleaned_comments'] = df.cleaned_comments.apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create an instance of the sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply the sentiment analyzer to each comment\n",
    "df['sentiment_scores'] = df['cleaned_comments'].apply(lambda x: sid.polarity_scores(x))\n",
    "\n",
    "# Extract the compound score from the sentiment scores\n",
    "df['compound_score'] = df['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Categorize the comments as positive, negative, or neutral based on the compound score\n",
    "df['sentiment_category'] = pd.cut(df['compound_score'], bins=3, labels=['negative', 'neutral', 'positive'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwordcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m WordCloud\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Join the words in the cleaned comments to a single string\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Join the words in the cleaned comments to a single string\n",
    "text = ' '.join(df['cleaned_comments'])\n",
    "\n",
    "# Create a word cloud object and generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.9.1.1.tar.gz (222 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\user\\google drive\\projects personal\\data science projects\\sentiment_analyis\\tech_review_sentiment\\.conda\\lib\\site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\google drive\\projects personal\\data science projects\\sentiment_analyis\\tech_review_sentiment\\.conda\\lib\\site-packages (from wordcloud) (9.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\google drive\\projects personal\\data science projects\\sentiment_analyis\\tech_review_sentiment\\.conda\\lib\\site-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\google drive\\projects personal\\data science projects\\sentiment_analyis\\tech_review_sentiment\\.conda\\lib\\site-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\google drive\\projects personal\\data science projects\\sentiment_analyis\\tech_review_sentiment\\.conda\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\google drive\\projects personal\\data science projects\\sentiment_analyis\\tech_review_sentiment\\.conda\\lib\\site-packages (from matplotlib->wordcloud) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\google drive\\projects personal\\data science projects\\sentiment_analyis\\tech_review_sentiment\\.conda\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\google drive\\projects personal\\data science projects\\sentiment_analyis\\tech_review_sentiment\\.conda\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Building wheels for collected packages: wordcloud\n",
      "  Building wheel for wordcloud (setup.py): started\n",
      "  Building wheel for wordcloud (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for wordcloud\n",
      "Failed to build wordcloud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [20 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\tokenization.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\_version.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\__init__.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\__main__.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\stopwords -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      UPDATING build\\lib.win-amd64-cpython-311\\wordcloud/_version.py\n",
      "      set build\\lib.win-amd64-cpython-311\\wordcloud/_version.py to '1.9.1.1'\n",
      "      running build_ext\n",
      "      building 'wordcloud.query_integral_image' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for wordcloud\n",
      "ERROR: Could not build wheels for wordcloud, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "# 👇️ make sure to use your version of Python, e.g. 3.10\n",
    "!python -m pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret input 'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m sns\u001b[39m.\u001b[39mset_style(\u001b[39m\"\u001b[39m\u001b[39mwhitegrid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m sns\u001b[39m.\u001b[39;49mcountplot(x\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msentiment\u001b[39;49m\u001b[39m'\u001b[39;49m, data\u001b[39m=\u001b[39;49mdf)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\seaborn\\categorical.py:2943\u001b[0m, in \u001b[0;36mcountplot\u001b[1;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, width, dodge, ax, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m \u001b[39melif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2941\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot pass values for both `x` and `y`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2943\u001b[0m plotter \u001b[39m=\u001b[39m _CountPlotter(\n\u001b[0;32m   2944\u001b[0m     x, y, hue, data, order, hue_order,\n\u001b[0;32m   2945\u001b[0m     estimator, errorbar, n_boot, units, seed,\n\u001b[0;32m   2946\u001b[0m     orient, color, palette, saturation,\n\u001b[0;32m   2947\u001b[0m     width, errcolor, errwidth, capsize, dodge\n\u001b[0;32m   2948\u001b[0m )\n\u001b[0;32m   2950\u001b[0m plotter\u001b[39m.\u001b[39mvalue_label \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2952\u001b[0m \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\seaborn\\categorical.py:1530\u001b[0m, in \u001b[0;36m_BarPlotter.__init__\u001b[1;34m(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, x, y, hue, data, order, hue_order,\n\u001b[0;32m   1526\u001b[0m              estimator, errorbar, n_boot, units, seed,\n\u001b[0;32m   1527\u001b[0m              orient, color, palette, saturation, width,\n\u001b[0;32m   1528\u001b[0m              errcolor, errwidth, capsize, dodge):\n\u001b[0;32m   1529\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize the plotter.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1530\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestablish_variables(x, y, hue, data, orient,\n\u001b[0;32m   1531\u001b[0m                              order, hue_order, units)\n\u001b[0;32m   1532\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestablish_colors(color, palette, saturation)\n\u001b[0;32m   1533\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_statistic(estimator, errorbar, n_boot, seed)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\seaborn\\categorical.py:541\u001b[0m, in \u001b[0;36m_CategoricalPlotter.establish_variables\u001b[1;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(var, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    540\u001b[0m         err \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not interpret input \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mvar\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 541\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[0;32m    543\u001b[0m \u001b[39m# Figure out the plotting orientation\u001b[39;00m\n\u001b[0;32m    544\u001b[0m orient \u001b[39m=\u001b[39m infer_orient(\n\u001b[0;32m    545\u001b[0m     x, y, orient, require_numeric\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire_numeric\n\u001b[0;32m    546\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret input 'sentiment'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.countplot(x='sentiment', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwordcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m WordCloud\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Concatenate all the comments into a single string\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concatenate all the comments into a single string\n",
    "all_comments = ' '.join(df['clean_comments'])\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=500, background_color='white', \n",
    "                      stopwords=stop_words, max_words=200).generate(all_comments)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analyis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
