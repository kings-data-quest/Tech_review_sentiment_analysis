{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import demoji\n",
    "\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9428\\183877869.py:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "demoji.download_codes()\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " def getComments(url):\n",
    "\n",
    "    # Set up Chrome driver options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    # Initialize ChromeDriver with an implicit wait of 20 seconds\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.implicitly_wait(50)\n",
    "\n",
    "    # Open the video page\n",
    "    driver.get(url)\n",
    "    sleep(7)\n",
    "\n",
    "    # Wait for the comment section to load\n",
    "    wait = WebDriverWait(driver, 40)\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"continuations\"]')))\n",
    "\n",
    "    scroll_pause_time = 10\n",
    "    scroll_count = 0\n",
    "    comment_list = []\n",
    "\n",
    "    while len(comment_list) < 50 and scroll_count < 10:\n",
    "        # Scroll down to the bottom of the page to load more comments\n",
    "        driver.execute_script('window.scrollTo(0, document.documentElement.scrollHeight);')\n",
    "        scroll_count += 1\n",
    "        sleep(scroll_pause_time)\n",
    "\n",
    "        # Find the comment div element and extract the comments\n",
    "        comment_div = driver.find_element(By.XPATH, '//*[@id=\"contents\"]')\n",
    "        comments = comment_div.find_elements(By.XPATH, '//*[@id=\"content-text\"]')\n",
    "        for comment in comments:\n",
    "            comment_list.append(comment.text.strip())\n",
    "            \n",
    "    # print(f\"Scraped {len(comment_list)} comments\")\n",
    "\n",
    "    driver.quit()\n",
    "    return comment_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.youtube.com/watch?v=zz70o2Ia4X0\", \"https://www.youtube.com/watch?v=XUkV3RsjGrs\",\"https://www.youtube.com/watch?v=UORoRpW8tnA\" ,\n",
    "        \"https://www.youtube.com/watch?v=CM7TbmdOeAs\", \"https://youtu.be/zhoTX0RRXPQ\", \"https://youtu.be/BZPV3Cf3pLo\",\"https://youtu.be/nlowwv-I81M\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [06:24<00:00, 54.97s/it]\n"
     ]
    }
   ],
   "source": [
    "comments = []\n",
    "for url in tqdm(urls):\n",
    "    com = getComments(url)\n",
    "    comments.extend(com)\n",
    "    sleep(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'comments':comments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('comments.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'emoji' from 'c:\\\\Users\\\\USER\\\\Google Drive\\\\PROJECTS PERSONAL\\\\Data Science projects\\\\Sentiment_analyis\\\\Tech_review_sentiment\\\\.conda\\\\Lib\\\\site-packages\\\\emoji\\\\__init__.py'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation marks\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers and special characters\n",
    "    text = re.sub('[^a-zA-Z ]', '', text)\n",
    "\n",
    "    # Remove URLs and mentions\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stem the words\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join the words back into a string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return demoji.replace(text, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USER/nltk_data'\n    - 'c:\\\\Users\\\\USER\\\\Google Drive\\\\PROJECTS PERSONAL\\\\Data Science projects\\\\Sentiment_analyis\\\\Tech_review_sentiment\\\\.conda\\\\nltk_data'\n    - 'c:\\\\Users\\\\USER\\\\Google Drive\\\\PROJECTS PERSONAL\\\\Data Science projects\\\\Sentiment_analyis\\\\Tech_review_sentiment\\\\.conda\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\USER\\\\Google Drive\\\\PROJECTS PERSONAL\\\\Data Science projects\\\\Sentiment_analyis\\\\Tech_review_sentiment\\\\.conda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[39m.\u001b[39;49mcomments\u001b[39m.\u001b[39;49mapply(clean_text)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\pandas\\core\\series.py:4626\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4516\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4517\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4518\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4521\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4522\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4523\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4524\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4525\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4624\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4625\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4626\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[23], line 19\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     16\u001b[0m text \u001b[39m=\u001b[39m remove_emojis(text)\n\u001b[0;32m     18\u001b[0m \u001b[39m# Tokenize the text into words\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m words \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     21\u001b[0m \u001b[39m# Remove stop words\u001b[39;00m\n\u001b[0;32m     22\u001b[0m words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\USER\\Google Drive\\PROJECTS PERSONAL\\Data Science projects\\Sentiment_analyis\\Tech_review_sentiment\\.conda\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USER/nltk_data'\n    - 'c:\\\\Users\\\\USER\\\\Google Drive\\\\PROJECTS PERSONAL\\\\Data Science projects\\\\Sentiment_analyis\\\\Tech_review_sentiment\\\\.conda\\\\nltk_data'\n    - 'c:\\\\Users\\\\USER\\\\Google Drive\\\\PROJECTS PERSONAL\\\\Data Science projects\\\\Sentiment_analyis\\\\Tech_review_sentiment\\\\.conda\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\USER\\\\Google Drive\\\\PROJECTS PERSONAL\\\\Data Science projects\\\\Sentiment_analyis\\\\Tech_review_sentiment\\\\.conda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "df.comments.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'emoji' has no attribute 'get_emoji_regexp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m clean_comments \u001b[39m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m comment \u001b[39min\u001b[39;00m comments:\n\u001b[1;32m----> 3\u001b[0m     clean_comment \u001b[39m=\u001b[39m clean_text(comment)\n\u001b[0;32m      4\u001b[0m     clean_comments\u001b[39m.\u001b[39mappend(clean_comment)\n\u001b[0;32m      6\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mclean_comments\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m clean_comments\n",
      "Cell \u001b[1;32mIn[28], line 16\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     13\u001b[0m text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39m'\u001b[39m\u001b[39m@[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, text)\n\u001b[0;32m     15\u001b[0m \u001b[39m# Remove emojis\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m text \u001b[39m=\u001b[39m emoji\u001b[39m.\u001b[39;49mget_emoji_regexp()\u001b[39m.\u001b[39msub(\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, text)\n\u001b[0;32m     18\u001b[0m \u001b[39m# Tokenize the text into words\u001b[39;00m\n\u001b[0;32m     19\u001b[0m words \u001b[39m=\u001b[39m word_tokenize(text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'emoji' has no attribute 'get_emoji_regexp'"
     ]
    }
   ],
   "source": [
    "clean_comments = []\n",
    "for comment in comments:\n",
    "    clean_comment = clean_text(comment)\n",
    "    clean_comments.append(clean_comment)\n",
    "\n",
    "df['clean_comments'] = clean_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analyis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
