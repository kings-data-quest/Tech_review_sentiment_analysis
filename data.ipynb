{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " def getComments(url):\n",
    "\n",
    "    # Set up Chrome driver options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    # Initialize ChromeDriver with an implicit wait of 20 seconds\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.implicitly_wait(20)\n",
    "\n",
    "    # Open the video page\n",
    "    driver.get(url)\n",
    "    sleep(7)\n",
    "\n",
    "    # Wait for the comment section to load\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"continuations\"]')))\n",
    "\n",
    "    scroll_pause_time = 10\n",
    "    scroll_count = 0\n",
    "    comment_list = []\n",
    "\n",
    "    while len(comment_list) < 50 and scroll_count < 10:\n",
    "        # Scroll down to the bottom of the page to load more comments\n",
    "        driver.execute_script('window.scrollTo(0, document.documentElement.scrollHeight);')\n",
    "        scroll_count += 1\n",
    "        sleep(scroll_pause_time)\n",
    "\n",
    "        # Find the comment div element and extract the comments\n",
    "        comment_div = driver.find_element(By.XPATH, '//*[@id=\"contents\"]')\n",
    "        comments = comment_div.find_elements(By.XPATH, '//*[@id=\"content-text\"]')\n",
    "        for comment in comments:\n",
    "            comment_list.append(comment.text.strip())\n",
    "            \n",
    "    # print(f\"Scraped {len(comment_list)} comments\")\n",
    "\n",
    "    driver.quit()\n",
    "    return comment_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.youtube.com/watch?v=zz70o2Ia4X0\", \"https://www.youtube.com/watch?v=XUkV3RsjGrs\",\"https://www.youtube.com/watch?v=UORoRpW8tnA\" ,\n",
    "        \"https://www.youtube.com/watch?v=CM7TbmdOeAs\", \"https://youtu.be/zhoTX0RRXPQ\", \"https://youtu.be/BZPV3Cf3pLo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [05:19<00:00, 53.19s/it]\n"
     ]
    }
   ],
   "source": [
    "comments = []\n",
    "for url in tqdm(urls):\n",
    "    com = getComments(url)\n",
    "    comments.extend(com)\n",
    "    sleep(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation marks\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers and special characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Remove URLs and mentions\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stem the words\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join the words back into a string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analyis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
